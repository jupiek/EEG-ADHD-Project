---
title: "Data Analysis"
author: "Julie Kim"
date: "04/08/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(ggplot2)
library(glmnet)
library(pander)
library(randomForest)
```
    
    
####Read in, Clean up

    + Group left/right (3x variable reduction)
    + Subtract baseline from activity (2x variable reduction)
    + Include both alpha/theta
    
    Read Data
    ```{r}
    #Read Data
    df = read.csv("ADHD_EEG_Dataset.csv")
    dim(df)
    
    #Subset of data to contain only the ID, response, and EEG readings
    #i.e. exclude demographic data
    df = df[,c(1,2,40:163)]
    
    #Omit all NA's
    df = na.omit(df)
    dim(df)
    ```
    
    Group Data
    ```{r}
    #Group the brain regions based on their position (left/right)
    #Method: Average reading
    #Odd numbers are left, even numbers right
    data = df[,1:2]
    
    #Baseline Activity - Alpha Band
    data$BS_LF_ALPHA = NA
    data$BS_RF_ALPHA = NA
    data$BS_LC_ALPHA = df$BS_C3_ALPHA
    data$BS_RC_ALPHA = df$BS_C4_ALPHA
    data$BS_LT_ALPHA = df$BS_T7_ALPHA
    data$BS_RT_ALPHA = df$BS_T8_ALPHA
    data$BS_LP_ALPHA = NA
    data$BS_RP_ALPHA = NA
    data$BS_LO_ALPHA = df$BS_O1_ALPHA
    data$BS_RO_ALPHA = df$BS_O2_ALPHA
    
    #Baseline Activity - Theta Band
    data$BS_LF_THETA = NA
    data$BS_RF_THETA = NA
    data$BS_LC_THETA = df$BS_C3_THETA
    data$BS_RC_THETA = df$BS_C4_THETA
    data$BS_LT_THETA = df$BS_T7_THETA
    data$BS_RT_THETA = df$BS_T8_THETA
    data$BS_LP_THETA = NA
    data$BS_RP_THETA = NA
    data$BS_LO_THETA = df$BS_O1_THETA
    data$BS_RO_THETA = df$BS_O2_THETA
    
    #Attention Task Activity - Alpha Band
    data$AT_LF_ALPHA = NA
    data$AT_RF_ALPHA = NA
    data$AT_LC_ALPHA = df$AT_C3_ALPHA
    data$AT_RC_ALPHA = df$AT_C4_ALPHA
    data$AT_LT_ALPHA = df$AT_T7_ALPHA
    data$AT_RT_ALPHA = df$AT_T8_ALPHA
    data$AT_LP_ALPHA = NA
    data$AT_RP_ALPHA = NA
    data$AT_LO_ALPHA = df$AT_O1_ALPHA
    data$AT_RO_ALPHA = df$AT_O2_ALPHA
    
    #Attention Task Activity - Theta Band
    data$AT_LF_THETA = NA
    data$AT_RF_THETA = NA
    data$AT_LC_THETA = df$AT_C3_THETA
    data$AT_RC_THETA = df$AT_C4_THETA
    data$AT_LT_THETA = df$AT_T7_THETA
    data$AT_RT_THETA = df$AT_T8_THETA
    data$AT_LP_THETA = NA
    data$AT_RP_THETA = NA
    data$AT_LO_THETA = df$AT_O1_THETA
    data$AT_RO_THETA = df$AT_O2_THETA
    
    for (i in 1:nrow(df)) {
      #Baseline Activity - Alpha Band
      data$BS_LF_ALPHA[i] = mean(df$BS_F1_ALPHA[i], df$BS_F3_ALPHA[i], df$BS_F7_ALPHA[i])
      data$BS_RF_ALPHA[i] = mean(df$BS_F2_ALPHA[i], df$BS_F4_ALPHA[i], df$BS_F8_ALPHA[i])
      data$BS_LP_ALPHA[i] = mean(df$BS_P7_ALPHA[i], df$BS_P3_ALPHA[i])
      data$BS_RP_ALPHA[i] = mean(df$BS_P8_ALPHA[i], df$BS_P4_ALPHA[i])
      #Baseline Activity - Theta Band
      data$BS_LF_THETA[i] = mean(df$BS_F1_THETA[i], df$BS_F3_THETA[i], df$BS_F7_THETA[i])
      data$BS_RF_THETA[i] = mean(df$BS_F2_THETA[i], df$BS_F4_THETA[i], df$BS_F8_THETA[i])
      data$BS_LP_THETA[i] = mean(df$BS_P7_THETA[i], df$BS_P3_THETA[i])
      data$BS_RP_THETA[i] = mean(df$BS_P8_THETA[i], df$BS_P4_THETA[i])
      
      #Attention Task Activity - Alpha Band
      data$AT_LF_ALPHA[i] = mean(df$AT_F1_ALPHA[i], df$AT_F3_ALPHA[i], df$AT_F7_ALPHA[i])
      data$AT_RF_ALPHA[i] = mean(df$AT_F2_ALPHA[i], df$AT_F4_ALPHA[i], df$AT_F8_ALPHA[i])
      data$AT_LP_ALPHA[i] = mean(df$AT_P7_ALPHA[i], df$AT_P3_ALPHA[i])
      data$AT_RP_ALPHA[i] = mean(df$AT_P8_ALPHA[i], df$AT_P4_ALPHA[i])
      #Attention Task Activity - Theta Band
      data$AT_LF_THETA[i] = mean(df$AT_F1_THETA[i], df$AT_F3_THETA[i], df$AT_F7_THETA[i])
      data$AT_RF_THETA[i] = mean(df$AT_F2_THETA[i], df$AT_F4_THETA[i], df$AT_F8_THETA[i])
      data$AT_LP_THETA[i] = mean(df$AT_P7_THETA[i], df$AT_P3_THETA[i])
      data$AT_RP_THETA[i] = mean(df$AT_P8_THETA[i], df$AT_P4_THETA[i])
    }
    
    #Baseline Connectivity - Alpha Band
    data$BS_LFF_ALPHA = NA
    data$BS_RFF_ALPHA = NA
    data$BS_LFC_ALPHA = df$BS_F3C3_ALPHA
    data$BS_RFC_ALPHA = df$BS_F4C4_ALPHA
    data$BS_LFP_ALPHA = NA
    data$BS_RFP_ALPHA = NA
    data$BS_LFT_ALPHA = df$BS_F3T7_ALPHA            
    data$BS_RFT_ALPHA = df$BS_F4T8_ALPHA
    data$BS_LFO_ALPHA = df$BS_F3O1_ALPHA
    data$BS_RFO_ALPHA = df$BS_F4O2_ALPHA
    
    #Baseline Connectivity - Theta Band
    data$BS_LFF_THETA = NA
    data$BS_RFF_THETA = NA
    data$BS_LFC_THETA = df$BS_F3C3_THETA
    data$BS_RFC_THETA = df$BS_F4C4_THETA
    data$BS_LFP_THETA = NA
    data$BS_RFP_THETA = NA
    data$BS_LFT_THETA = df$BS_F3T7_THETA            
    data$BS_RFT_THETA = df$BS_F4T8_THETA
    data$BS_LFO_THETA = df$BS_F3O1_THETA
    data$BS_RFO_THETA = df$BS_F4O2_THETA
    
    #Attention Task Connectivity - Alpha Band
    data$AT_LFF_ALPHA = NA
    data$AT_RFF_ALPHA = NA
    data$AT_LFC_ALPHA = df$AT_F3C3_ALPHA
    data$AT_RFC_ALPHA = df$AT_F4C4_ALPHA
    data$AT_LFP_ALPHA = NA
    data$AT_RFP_ALPHA = NA
    data$AT_LFT_ALPHA = df$AT_F3T7_ALPHA            
    data$AT_RFT_ALPHA = df$AT_F4T8_ALPHA
    data$AT_LFO_ALPHA = df$AT_F3O1_ALPHA
    data$AT_RFO_ALPHA = df$AT_F4O2_ALPHA
    
    #Attention Task Connectivity - Theta Band
    data$AT_LFF_THETA = NA
    data$AT_RFF_THETA = NA
    data$AT_LFC_THETA = df$AT_F3C3_THETA
    data$AT_RFC_THETA = df$AT_F4C4_THETA
    data$AT_LFP_THETA = NA
    data$AT_RFP_THETA = NA
    data$AT_LFT_THETA = df$AT_F3T7_THETA            
    data$AT_RFT_THETA = df$AT_F4T8_THETA
    data$AT_LFO_THETA = df$AT_F3O1_THETA
    data$AT_RFO_THETA = df$AT_F4O2_THETA
    
    for (i in 1:nrow(df)) {
      #Baseline Connectivity - Alpha Band
      data$BS_LFF_ALPHA[i] = mean(df$BS_F1F3_ALPHA[i], df$BS_F3F7_ALPHA[i])
      data$BS_RFF_ALPHA[i] = mean(df$BS_F2F4_ALPHA[i], df$BS_F4F8_ALPHA[i])
      data$BS_LFP_ALPHA[i] = mean(df$BS_F3P3_ALPHA[i], df$BS_F3P7_ALPHA[i])
      data$BS_RFP_ALPHA[i] = mean(df$BS_F4P8_ALPHA[i], df$BS_F4P4_ALPHA[i])
      #Baseline Connectivity - Theta Band
      data$BS_LFF_THETA[i] = mean(df$BS_F1F3_THETA[i], df$BS_F3F7_THETA[i])
      data$BS_RFF_THETA[i] = mean(df$BS_F2F4_THETA[i], df$BS_F4F8_THETA[i])
      data$BS_LFP_THETA[i] = mean(df$BS_F3P3_THETA[i], df$BS_F3P7_THETA[i])
      data$BS_RFP_THETA[i] = mean(df$BS_F4P8_THETA[i], df$BS_F4P4_THETA[i])
      
      #Attention Task Connectivity - Alpha Band
      data$AT_LFF_ALPHA[i] = mean(df$AT_F1F3_ALPHA[i], df$AT_F3F7_ALPHA[i])
      data$AT_RFF_ALPHA[i] = mean(df$AT_F2F4_ALPHA[i], df$AT_F4F8_ALPHA[i])
      data$AT_LFP_ALPHA[i] = mean(df$AT_F3P3_ALPHA[i], df$AT_F3P7_ALPHA[i])
      data$AT_RFP_ALPHA[i] = mean(df$AT_F4P8_ALPHA[i], df$AT_F4P4_ALPHA[i])
      #Attention Task Connectivity - Theta Band
      data$AT_LFF_THETA[i] = mean(df$AT_F1F3_THETA[i], df$AT_F3F7_THETA[i])
      data$AT_RFF_THETA[i] = mean(df$AT_F2F4_THETA[i], df$AT_F4F8_THETA[i])
      data$AT_LFP_THETA[i] = mean(df$AT_F3P3_THETA[i], df$AT_F3P7_THETA[i])
      data$AT_RFP_THETA[i] = mean(df$AT_F4P8_THETA[i], df$AT_F4P4_THETA[i])
    }
    
    data$BS_time = df$BS_time
    data$BS_dft = df$BS_dft
    data$AT_time = df$AT_time
    data$AT_dft = df$AT_dft
    ```
    
    Subtract Baseline from Activity
    + The dataset with only attention level is saved as "data.ATT"
    + The change of the activity/connectivity from baseline are all saved to a new data frame calld "data.change"
    ```{r}
        # Enforce consecutive row name reference
    rownames(data) <- NULL

    # Subtract baseline activity/ connectivity signal from activity
    data.change <- mutate(data,
                    # EEG Activity Subtraction
                    LF_ALPHA = AT_LF_ALPHA - BS_LF_ALPHA,
                    RF_ALPHA = AT_RF_ALPHA - BS_RF_ALPHA,
                    LC_ALPHA = AT_LC_ALPHA - BS_LC_ALPHA,
                    RC_ALPHA = AT_RC_ALPHA - BS_RC_ALPHA,
                    LT_ALPHA = AT_LT_ALPHA - BS_LT_ALPHA,
                    RT_ALPHA = AT_RT_ALPHA - BS_RT_ALPHA,
                    LP_ALPHA = AT_LP_ALPHA - BS_LP_ALPHA,
                    RP_ALPHA = AT_RP_ALPHA - BS_RP_ALPHA,
                    LO_ALPHA = AT_LO_ALPHA - BS_LO_ALPHA,
                    RO_ALPHA = AT_RO_ALPHA - BS_RO_ALPHA,
                    LF_THETA = AT_LF_THETA - BS_LF_THETA,
                    RF_THETA = AT_RF_THETA - BS_RF_THETA,
                    LC_THETA = AT_LC_THETA - BS_LC_THETA,
                    RC_THETA = AT_RC_THETA - BS_RC_THETA,
                    LT_THETA = AT_LT_THETA - BS_LT_THETA,
                    RT_THETA = AT_RT_THETA - BS_RT_THETA,
                    LP_THETA = AT_LP_THETA - BS_LP_THETA,
                    RP_THETA = AT_RP_THETA - BS_RP_THETA,
                    LO_THETA = AT_LO_THETA - BS_LO_THETA,
                    RO_THETA = AT_RO_THETA - BS_RO_THETA,
                    # EEG Connectivity subtraction
                    LFF_ALPHA = AT_LFF_ALPHA - BS_LFF_ALPHA,
                    RFF_ALPHA = AT_RFF_ALPHA - BS_RFF_ALPHA,
                    LFC_ALPHA = AT_LFC_ALPHA - BS_LFC_ALPHA,
                    RFC_ALPHA = AT_RFC_ALPHA - BS_RFC_ALPHA,
                    LFP_ALPHA = AT_LFP_ALPHA - BS_LFP_ALPHA,
                    RFP_ALPHA = AT_RFP_ALPHA - BS_RFP_ALPHA,
                    LFT_ALPHA = AT_LFT_ALPHA - BS_LFT_ALPHA,
                    RFT_ALPHA = AT_RFT_ALPHA - BS_RFT_ALPHA,
                    LFO_ALPHA = AT_LFO_ALPHA - BS_LFO_ALPHA,
                    RFO_ALPHA = AT_RFO_ALPHA - BS_RFO_ALPHA,
                    LFF_THETA = AT_LFF_THETA - BS_LFF_THETA,
                    RFF_THETA = AT_RFF_THETA - BS_RFF_THETA,
                    LFC_THETA = AT_LFC_THETA - BS_LFC_THETA,
                    RFC_THETA = AT_RFC_THETA - BS_RFC_THETA,
                    LFP_THETA = AT_LFP_THETA - BS_LFP_THETA,
                    RFP_THETA = AT_RFP_THETA - BS_RFP_THETA,
                    LFT_THETA = AT_LFT_THETA - BS_LFT_THETA,
                    RFT_THETA = AT_RFT_THETA - BS_RFT_THETA,
                    LFO_THETA = AT_LFO_THETA - BS_LFO_THETA,
                    RFO_THETA = AT_RFO_THETA - BS_RFO_THETA,
                    time = AT_time - BS_time,
                    dft = AT_dft - BS_dft
    )
    
    # Groupped left/right brain regions
    data
    dim(data)
    
    # Change scores by subtracting baseline values from attention task values
    data.change = data.change[,-c(3:86)]
    data.change
    dim(data.change)
    
    # Just EEG Activity
    data.ACT = data[,-c(43:86)]
    data.ACT
    dim(data.ACT)
    
    # Just EEG Connectivity
    data.CON = data[,c(1:2,43:82)]
    data.CON
    dim(data.CON)
    
    # Just EEG Connectivity + Activity without baseline
    data.ATT = data[,c(1,2,23:42,63:82)]
    data.ATT
    dim(data.ATT)
    
    # Just data from the alpha band
    data.alpha = data[,c(1:12,23:32,43:52,63:72)]
    data.alpha
    dim(data.alpha)
    
    # Just data from the theta band
    data.theta = data[,-c(3:12,23:32,43:52,63:72,83:86)]
    data.theta
    dim(data.theta)
    ```

    Scale Data
    + Normalized original dataset is saved as "data.ATT.norm"
    + Normalized change score dataset is saved as "data.change.norm"
    ```{r, message=F, warning=F}
    #Scale data to be centered at 0, with standard deviation 1
    
    # Normalize the original dataset "data"
    data.norm = as.data.frame(cbind(ID = data[,1], ADHD = data[,2], scale(data[,-c(1,2)])))
    data.norm = mutate(data.norm, ID = data[,1])

    #Check that dataset is normalized
    checkMat = round(colMeans(data.norm[,-c(1,2)]),10)==0
    ifelse(length(checkMat[checkMat==TRUE])/length(checkMat)==1,
          print("Data to be centered at 0 success"),
          print("Data to be centered at 0 fail"))
    checkMat2 = round(apply(data.norm[,-c(1,2)], 2, sd),10)==1
    ifelse(length(checkMat2[checkMat2==TRUE])/length(checkMat2)==1,
          print("Data to have standard deviation of 1 success"),
          print("Data to have standard deviation of 1 fail"))
    
    #Normalize the change score dataset "data.change"
    data.change.norm = as.data.frame(cbind(ID = data.change[,1], ADHD = data.change[,2], scale(data.change[,-c(1,2)])))
    data.change.norm = mutate(data.change.norm, ID = data.change[,1])

    #Check that dataset is normalized
    checkMat = round(colMeans(data.change.norm[,-c(1,2)]),10)==0
    ifelse(length(checkMat[checkMat==TRUE])/length(checkMat)==1,
          print("Data to be centered at 0 success"),
          print("Data to be centered at 0 fail"))
    checkMat2 = round(apply(data.change.norm[,-c(1,2)], 2, sd),10)==1
    ifelse(length(checkMat2[checkMat2==TRUE])/length(checkMat2)==1,
          print("Data to have standard deviation of 1 success"),
          print("Data to have standard deviation of 1 fail"))

    # Normalize just EEG Activity
    data.ACT.norm = as.data.frame(cbind(ID = data.ACT[,1], ADHD = data.ACT[,2], scale(data.ACT[,-c(1,2)])))
    data.ACT.norm = mutate(data.ACT.norm, ID = data.ACT[,1])

    #Check that dataset is normalized
    checkMat = round(colMeans(data.ACT.norm[,-c(1,2)]),10)==0
    ifelse(length(checkMat[checkMat==TRUE])/length(checkMat)==1,
          print("Data to be centered at 0 success"),
          print("Data to be centered at 0 fail"))
    checkMat2 = round(apply(data.ACT.norm[,-c(1,2)], 2, sd),10)==1
    ifelse(length(checkMat2[checkMat2==TRUE])/length(checkMat2)==1,
          print("Data to have standard deviation of 1 success"),
          print("Data to have standard deviation of 1 fail"))
    
    # Just EEG Connectivity
    data.CON.norm = as.data.frame(cbind(ID = data.CON[,1], ADHD = data.CON[,2], scale(data.CON[,-c(1,2)])))
    data.CON.norm = mutate(data.CON.norm, ID = data.CON[,1])

    #Check that dataset is normalized
    checkMat = round(colMeans(data.CON.norm[,-c(1,2)]),10)==0
    ifelse(length(checkMat[checkMat==TRUE])/length(checkMat)==1,
          print("Data to be centered at 0 success"),
          print("Data to be centered at 0 fail"))
    checkMat2 = round(apply(data.CON.norm[,-c(1,2)], 2, sd),10)==1
    ifelse(length(checkMat2[checkMat2==TRUE])/length(checkMat2)==1,
          print("Data to have standard deviation of 1 success"),
          print("Data to have standard deviation of 1 fail"))
    
    # Normalize just EEG Connectivity + Activity without baseline
    data.ATT.norm = as.data.frame(cbind(ID = data.ATT[,1], ADHD = data.ATT[,2], scale(data.ATT[,-c(1,2)])))
    data.ATT.norm = mutate(data.norm, ID = data[,1])
    
    #Check that dataset is normalized
    checkMat = round(colMeans(data.ATT.norm[,-c(1,2)]),10)==0
    ifelse(length(checkMat[checkMat==TRUE])/length(checkMat)==1,
          print("Data to be centered at 0 success"),
          print("Data to be centered at 0 fail"))
    checkMat2 = round(apply(data.ATT.norm[,-c(1,2)], 2, sd),10)==1
    ifelse(length(checkMat2[checkMat2==TRUE])/length(checkMat2)==1,
          print("Data to have standard deviation of 1 success"),
          print("Data to have standard deviation of 1 fail"))
    
    # Just data from the alpha band
    data.alpha.norm = as.data.frame(cbind(ID = data.alpha[,1], ADHD = data.alpha[,2], scale(data.alpha[,-c(1,2)])))
    data.alpha.norm = mutate(data.alpha.norm, ID = data.alpha[,1])

    #Check that dataset is normalized
    checkMat = round(colMeans(data.alpha.norm[,-c(1,2)]),10)==0
    ifelse(length(checkMat[checkMat==TRUE])/length(checkMat)==1,
          print("Data to be centered at 0 success"),
          print("Data to be centered at 0 fail"))
    checkMat2 = round(apply(data.alpha.norm[,-c(1,2)], 2, sd),10)==1
    ifelse(length(checkMat2[checkMat2==TRUE])/length(checkMat2)==1,
          print("Data to have standard deviation of 1 success"),
          print("Data to have standard deviation of 1 fail"))
    
    # Just data from the theta band
    data.theta.norm = as.data.frame(cbind(ID = data.theta[,1], ADHD = data.theta[,2], scale(data.theta[,-c(1,2)])))
    data.theta.norm = mutate(data.theta.norm, ID = data.theta[,1])

    #Check that dataset is normalized
    checkMat = round(colMeans(data.theta.norm[,-c(1,2)]),10)==0
    ifelse(length(checkMat[checkMat==TRUE])/length(checkMat)==1,
          print("Data to be centered at 0 success"),
          print("Data to be centered at 0 fail"))
    checkMat2 = round(apply(data.theta.norm[,-c(1,2)], 2, sd),10)==1
    ifelse(length(checkMat2[checkMat2==TRUE])/length(checkMat2)==1,
          print("Data to have standard deviation of 1 success"),
          print("Data to have standard deviation of 1 fail"))
    
    # Print Data Set of interest and its dimension
    data.norm
    data.change.norm
    data.ACT.norm
    data.CON.norm
    data.ATT.norm
    data.alpha.norm
    data.alpha.norm
    dim(data.norm)
    dim(data.change.norm)
    dim(data.ACT.norm)
    dim(data.CON.norm)
    dim(data.ATT.norm)
    dim(data.alpha.norm)
    dim(data.theta.norm)
    ```


###Diagnostics

    PCA

```{r}
prout.norm = prcomp(data.norm[,-c(1:2)], scale=TRUE, center=TRUE)
prout.change.norm = prcomp(data.change.norm[,-c(1:2)], scale=TRUE, center=TRUE)
prout.ACT.norm = prcomp(data.ACT.norm[,-c(1:2)], scale=TRUE, center=TRUE)
prout.CON.norm = prcomp(data.CON.norm[,-c(1:2)], scale=TRUE, center=TRUE)
prout.ATT.norm = prcomp(data.ATT.norm[,-c(1:2)], scale=TRUE, center=TRUE)
prout.alpha.norm = prcomp(data.alpha.norm[,-c(1:2)], scale=TRUE, center=TRUE)
prout.theta.norm = prcomp(data.theta.norm[,-c(1:2)], scale=TRUE, center=TRUE)
```

```{r}
# Proportion of Variance Explained by Normal Data
# Cumulative Proportion of Variance Explained by Normal Data
# Screeplot
par(mfrow=c(2,2))
prvar.norm = prout.norm$sdev^2
pve.norm = prvar.norm/sum(prvar.norm)
plot(pve.norm, xlab="Principal Component", ylab ="",
     main = "Prop. of Var.")
plot(cumsum(pve.norm), xlab="Principal Component", ylab ="",
     main = "Cum. Prop. of Var.")
screeplot(prout.norm, type="lines", main = "Screeplot")

# Proportion of Variance Explained by Changed Normal Data
# Cumulative Proportion of Variance Explained by Changed Normal Data
# Screeplot
par(mfrow=c(2,2))
prvar.change.norm = prout.change.norm$sdev^2
pve.change.norm = prvar.change.norm/sum(prvar.change.norm)
plot(pve.change.norm, xlab="Principal Component", ylab ="",
     main = "Prop. of Var.")
plot(cumsum(pve.change.norm), xlab="Principal Component", ylab ="",
     main = "Cum. Prop. of Var.")
screeplot(prout.change.norm, type="lines", main = "Screeplot")


# Proportion of Variance Explained by ACT Data
# Cumulative Proportion of Variance Explained by ACT Data
# Screeplot
par(mfrow=c(2,2))
prvar.ACT.norm = prout.ACT.norm$sdev^2
pve.ACT.norm = prvar.ACT.norm/sum(prvar.ACT.norm)
plot(pve.ACT.norm, xlab="Principal Component", ylab ="",
     main = "Prop. of Var.")
plot(cumsum(pve.ACT.norm), xlab="Principal Component", ylab ="",
     main = "Cum. Prop. of Var.")
screeplot(prout.ACT.norm, type="lines", main = "Screeplot")

# Proportion of Variance Explained by CON Data
# Cumulative Proportion of Variance Explained by CON Data
# Screeplot
par(mfrow=c(2,2))
prvar.CON.norm = prout.CON.norm$sdev^2
pve.CON.norm = prvar.CON.norm/sum(prvar.CON.norm)
plot(pve.CON.norm, xlab="Principal Component", ylab ="",
     main = "Prop. of Var.")
plot(cumsum(pve.CON.norm), xlab="Principal Component", ylab ="",
     main = "Cum. Prop. of Var.")
screeplot(prout.CON.norm, type="lines", main = "Screeplot")


# Proportion of Variance Explained by ATT Data
# Cumulative Proportion of Variance Explained by ATT Data
# Screeplot
par(mfrow=c(2,2))
prvar.ATT.norm = prout.ATT.norm$sdev^2
pve.ATT.norm = prvar.ATT.norm/sum(prvar.ATT.norm)
plot(pve.ATT.norm, xlab="Principal Component", ylab ="",
     main = "Prop. of Var.")
plot(cumsum(pve.ATT.norm), xlab="Principal Component", ylab ="",
     main = "Cum. Prop. of Var.")
screeplot(prout.ATT.norm, type="lines", main = "Screeplot")


# Proportion of Variance Explained by alpha Data
# Cumulative Proportion of Variance Explained by alpha Data
# Screeplot
par(mfrow=c(2,2))
prvar.alpha.norm = prout.alpha.norm$sdev^2
pve.alpha.norm = prvar.alpha.norm/sum(prvar.alpha.norm)
plot(pve.alpha.norm, xlab="Principal Component", ylab ="",
     main = "Prop. of Var.")
plot(cumsum(pve.alpha.norm), xlab="Principal Component", ylab ="",
     main = "Cum. Prop. of Var.")
screeplot(prout.alpha.norm, type="lines", main = "Screeplot")

# Proportion of Variance Explained by theta Data
# Cumulative Proportion of Variance Explained by theta Data
# Screeplot
par(mfrow=c(2,2))
prvar.theta.norm = prout.theta.norm$sdev^2
pve.theta.norm = prvar.theta.norm/sum(prvar.theta.norm)
plot(pve.theta.norm, xlab="Principal Component", ylab ="",
     main = "Prop. of Var.")
plot(cumsum(pve.theta.norm), xlab="Principal Component", ylab ="",
     main = "Cum. Prop. of Var.")
screeplot(prout.theta.norm, type="lines", main = "Screeplot")
```

```{r}
# Bi-plots of Principal Component Scores for all data
biplot(prout.norm, scale=0)
biplot(prout.change.norm, scale=0)
biplot(prout.ACT.norm, scale=0)
biplot(prout.CON.norm, scale=0)
biplot(prout.ATT.norm, scale=0)
biplot(prout.alpha.norm, scale=0)
biplot(prout.theta.norm, scale=0)
```


###Modeling
    
    Base Misclassification Rate - we use this is a comparison to the models. 
    "If we guessed that all subjects did not have ADHD, how would we perform /
    how often would we be incorrect?"
    Every model under consideration should perform *at least* better than this 
    base misclassification rate. If not, don't even bother!
    ```{r}
    br = function(data) {return(mean(data$ADHD == 0))}
    base = br(data.norm)
    br(data.change.norm)
    br(data.ACT.norm)
    br(data.CON.norm)
    br(data.ATT.norm)
    br(data.alpha.norm)
    br(data.theta.norm)
    ```
    
####Elastic Net (Lasso and Ridge Regression Combined)

    Model fitting &  Cross Validation (number of folds should be specified)
    ```{r}
    #Cross Validation and Test Error - specify the number of folds
    ###WANHE & JAE'S CODE HERE
    
    test_error <- function(data){
      #leave out 10% of test data and get the misclassification rate
      n <- nrow(data)
      m <- round(.1*n)
      test_er<- numeric(0)
      train_er <- numeric(0)
      for(i in 1:100){
        test_indx <- sample(1:n, m, replace=FALSE)
        test <- data[test_indx,]
        train <- data[-test_indx,]
        #cross validation
        cvfit <- cv.glmnet(y=train$ADHD, x=as.matrix(train[, -c(1,2)]), 
                           family="binomial", alpha=0.7, 
                           type.measure = "class", nfolds=5)
        test_er[i] <- mean(predict(cvfit, s="lambda.min", type="class", 
                                   newx = as.matrix(test[, -c(1,2)])) != test$ADHD)
        train_er[i] <- mean(predict(cvfit, s="lambda.min", type="class", 
                                    newx = as.matrix(train[, -c(1,2)])) != train$ADHD)
      
      }
      return(list(train_er, test_er))
    }
    ```
    ```{r, cache = TRUE, autodep = TRUE}
    set.seed(100)
    mis_rate_norm <- test_error(data.norm)
    mis_rate_change_norm <- test_error(data.change.norm)
    mis_rate_act_norm <- test_error(data.ACT.norm)
    mis_rate_con_norm <- test_error(data.CON.norm)
    mis_rate_att_norm <- test_error(data.ATT.norm)
    mis_rate_alpha_norm <- test_error(data.alpha.norm)
    mis_rate_theta_norm <- test_error(data.theta.norm)
    ```
    ```{r}
    set.seed(100)
    mean(mis_rate_norm[[1]])
    mean(mis_rate_norm[[2]])
    
    mean(mis_rate_change_norm[[1]])
    mean(mis_rate_change_norm[[2]])
    mean(mis_rate_act_norm[[1]])
    mean(mis_rate_act_norm[[2]])
    mean(mis_rate_con_norm[[1]])
    mean(mis_rate_con_norm[[2]])
    mean(mis_rate_att_norm[[1]])
    mean(mis_rate_att_norm[[2]])
    
    mean(mis_rate_alpha_norm[[1]])
    mean(mis_rate_alpha_norm[[2]])
    mean(mis_rate_theta_norm[[1]])
    mean(mis_rate_theta_norm[[2]])
    
    plot(c(1:7), c(mean(mis_rate_norm[[1]]), mean(mis_rate_change_norm[[1]]), 
                   mean(mis_rate_act_norm[[1]]), mean(mis_rate_con_norm[[1]]), 
                   mean(mis_rate_att_norm[[1]]), mean(mis_rate_alpha_norm[[1]]), 
                   mean(mis_rate_theta_norm[[1]])), ylim=c(0.1, 0.68), 
         ylab="misclassification error", type="l", xlab="dataset", col="blue")
    lines(c(1:7), c(mean(mis_rate_norm[[2]]), mean(mis_rate_change_norm[[2]]), 
                    mean(mis_rate_act_norm[[2]]), mean(mis_rate_con_norm[[2]]), 
                    mean(mis_rate_att_norm[[2]]), mean(mis_rate_alpha_norm[[2]]),
                    mean(mis_rate_theta_norm[[2]])), col="red")
    legend("topright",legend = c("test error", "train error"), 
           col = c("red", "blue"), lty=1, cex=0.8)
    ```
    
    Diagnosis for second full dataset
    ```{r}
    cvfit <- cv.glmnet(y=data.change.norm$ADHD, 
                       x=as.matrix(data.change.norm[, -c(1,2)]), 
                       family="binomial", alpha=0.7, 
                       type.measure = "class", nfolds=5)
    coef(cvfit)
    ```
    
    Tuning alpha: Run through alpha values from 0 to 1 in increments of 0.1. 
    Return a table of alpha values and the number of times out of 500 trials 
    that it produces the minimum misclassification error. The alpha values that 
    minimize the misclassification error for the `data.change.norm` dataset 
    are 0.8 and 0.9. 
    ```{r, cache = TRUE, autodep = TRUE}
    set.seed(100)
    minimums = vector()
    for(i in 1:500) {
      ms = vector(length = 11)
      names(ms) = seq(0, 1, by = 0.1)
      for(a in seq(0, 1, by = 0.1)) {
      er.fit = glmnet(x = as.matrix(data.change.norm[,-c(1,2)]), 
                    y = data.change.norm$ADHD, family = "binomial", alpha = a)
      cv.fit = cv.glmnet(x = as.matrix(data.change.norm[,-c(1,2)]), 
                       y = data.change.norm$ADHD, family = "binomial", 
                       alpha = a, type.measure = "class", nfolds = 5)
      ms[a*10+1] = signif(cv.fit$cvm[which(cv.fit$lambda == cv.fit$lambda.min)], 2)
      }
      minimums = c(minimums, names(ms)[ms == min(ms)])
    }
    sort(table(as.numeric(minimums)))
    #returns best alpha
    #0 0.1 0.3 0.2 0.5 0.4 0.6 0.7   1 0.8 0.9
    #8  21  42  47  57  60  72  73  94  97  97
    ```
    
    Analysis with alpha = 0.9
    ```{r}
    set.seed(100)
    #Fit Elastic Net regression and choose significant classifiers/ predictors
    #Cross Validation - specify the number of folds
    er.fit = glmnet(x = as.matrix(data.change.norm[,-c(1,2)]), 
                    y = data.change.norm$ADHD, family = "binomial", alpha = 0.9)
    
    cv.fit = cv.glmnet(x = as.matrix(data.change.norm[,-c(1,2)]), 
                       y = data.change.norm$ADHD, family = "binomial", 
                       alpha = 0.9, type.measure = "class", nfolds = 5)
    
    #The optimal value of Î», by 5-fold cross-validation, is 0.075. 
    #The MSE with this penalty is 0.34.
    signif(cv.fit$lambda.min, 2)
    signif(cv.fit$cvm[which(cv.fit$lambda == cv.fit$lambda.min)], 2)
    
    #Plot of lambdas with vertical line at minimizing lambda
    plot.glmnet(er.fit, "lambda")
    abline(v = log(cv.fit$lambda.min), col = "dimgray")
    ```

    Graphic Analysis
    ```{r}
    set.seed(100)
    # Model Predictions vs. True Observations
    er.pred = predict(cv.fit, newx = as.matrix(data.change.norm[,-c(1,2)]), 
                      s = "lambda.min", type = "response")
    plot(data.change.norm$ADHD, er.pred, 
         main = "Model Predictions vs. True Observations",
         xlab = "Diagnosis", ylab = "Predictions")
    ```
    
    Bootstrapping
    ```{r}
    #Bootstrapping code revised based on code from ADAFAEPOV Chapter 6, Cosma Shalizi
    resample = function(x) {sample(x, size = length(x), replace = TRUE)}
        
    resample.data.frame = function(data) {
      sample.rows = resample(1:nrow(data))
      return(data[sample.rows, ])
    }
    
    rboot = function(statistic, simulator, B) {
      tboots = replicate(B, statistic(simulator()))
      if (is.null(dim(tboots))) {
        tboots = array(tboots, dim = c(1, B))
      }
      return(tboots)
    }
    
    bootstrap = function(tboots, summarizer, ...) {
      summaries = apply(tboots, 1, summarizer, ...)
      return(t(summaries))
    }
    
    bootstrap.se = function(statistic, simulator, B) {
      bootstrap(rboot(statistic, simulator, B), summarizer = sd)
    }
    
    equitails = function(x, alpha) {
      lower = quantile(x, alpha/2)
      upper = quantile(x, 1 - alpha/2)
      return(c(lower, upper))
    }
        
    bootstrap.ci = function(statistic = NULL, simulator = NULL, tboots = NULL, 
                            B = if (!is.null(tboots)) {ncol(tboots)}, 
                            t.hat, level) {
      
      if (is.null(tboots)) {
        stopifnot(!is.null(statistic))
        stopifnot(!is.null(simulator))
        stopifnot(!is.null(B))
        tboots = rboot(statistic, simulator, B)
      }
      
      alpha = 1 - level
      intervals = bootstrap(tboots, summarizer = equitails, alpha = alpha)
      upper = t.hat + (t.hat - intervals[, 1])
      lower = t.hat + (t.hat - intervals[, 2])
      CIs = cbind(lower = lower, upper = upper)
      
      return(CIs)
      
    }
    ```
    ```{r, include = FALSE, warning = FALSE, message = FALSE}
    #Function mc.cis()
    #Returns the a-level CI of an elastic-net model's misclassification rates
    #Method of Prediction: Case Resampling
    #Input
      #Data frame on stocks with dates and stock prices or returns (df)
      #Alpha level for confidence interval (a)
    #Output
      #Misclassification Error
    
    mc.cis = function(df, a = 0.95) {
      
      en.mdl = cv.glmnet(y = df$ADHD, x = as.matrix(df[,-c(1,2)]), 
                         family = "binomial", alpha = 0.9, 
                         type.measure = "class", nfolds = 5)
      mc = mean(predict(en.mdl, s = "lambda.min", type = "class", 
                        newx = as.matrix(df[,-c(1,2)])) != df$ADHD)
        
      resample = function() {resample.data.frame(df)}
          
      estimator = function(data) {
        en.mdl = cv.glmnet(y = data$ADHD, x = as.matrix(data[,-c(1,2)]), 
                           family = "binomial", alpha = 0.9, 
                           type.measure = "class", nfolds = 5)
        mc =  mean(predict(en.mdl, s = "lambda.min", type = "class", 
                        newx = as.matrix(data[,-c(1,2)])) != data$ADHD)
        return(mc)
      }
      
      mc.cis = bootstrap.ci(statistic = estimator,
                            simulator = resample, 
                            B = 1000, t.hat = mc, 
                            level = a)
      
      return(mc.cis)
      
    }
    ```
    ```{r, cache = TRUE, autodep = TRUE}
    set.seed(100)
    #Misclassification Rate - Confidence Intervals by Bootstrapping over 1000 iterations
    #95%
    mc.cis(data.change.norm)
    #90%
    mc.cis(data.change.norm, 0.9)
    ```

####Random Forest

    Model fitting &  Cross Validation (number of folds should be specified)
    ```{r}
    set.seed(100)
    #diagnosis for second full dataset
    rf.mdl = randomForest(x = as.matrix(data.change.norm[,-c(1,2)]),
                          y = data.change.norm$ADHD,
                          ntree = 600)
    plot(data.change.norm$ADHD, predict(rf.mdl), 
         main = "Model Predictions vs. True Observations",
         xlab = "Diagnosis", ylab = "Predictions")
    
    #Error vs. # of Trees
    plot(rf.mdl)
    
    #roughly 45% misclassification
    mean(ifelse(predict(rf.mdl) > 0.5, 1, 0) != data.change.norm$ADHD)
    
    pred = predict(rf.mdl)
    CM = table(pred, data.change.norm$ADHD)
    
    #MSE = 0.2423
    mean(rf.mdl$mse)
    
    #Variable Importance Plot
    varImpPlot(rf.mdl, sort = T, n.var = 10,
               main = "Top 10 - Variable Importance")
    ```
    ```{r, message = FALSE, warning = FALSE, cache = TRUE, autodep = TRUE}
    set.seed(100)
    mc.rf.pred = function(data, threshold = 0.5) {
      mdl = randomForest(x = as.matrix(data[,-c(1,2)]),
                         y = data$ADHD,
                         ntree = 600)
      pred = predict(mdl)
      
      #1. Return probability that class = 1
      #2. Threshold said probabilities to output 0 or 1; default is 0.5
      #ROC Curve can help define those threholds
        #grid from 0 to 1, increments of 0.1
        #try picking the right parameter the minimizes training error the most
      mc = numeric(0)
      for(i in seq(0, 1, by = 0.1)) {
        mc[i*10+1] = mean(ifelse(pred > i, 1, 0) != data$ADHD)
      }

      plot(seq(0, 1, by = 0.1), mc, type = "l", 
           main = deparse(substitute(data)), 
           xlab = "Threshold", ylab = "Misclassification Rate")
      return(mean(ifelse(pred > threshold, 1, 0) != data$ADHD))
    }
    
    mc.rf.pred(data.norm, 0.7)
    mc.rf.pred(data.change.norm, 0.5)
    mc.rf.pred(data.ACT.norm, 0.7)
    mc.rf.pred(data.CON.norm, 0.8)
    mc.rf.pred(data.ATT.norm, 0.7)
    mc.rf.pred(data.alpha.norm, 0.8)
    mc.rf.pred(data.theta.norm, 0.6)
    ```
    ```{r, message = FALSE, warning = FALSE, cache = TRUE, autodep = TRUE}
    mc.rf = function(data) {
      n <- nrow(data)
      m <- round(.1*n)
      error = numeric(0)
      
      for(i in 1:100){
        test_indx <- sample(1:n, m, replace=FALSE)
        test <- data[test_indx,]
        train <- data[-test_indx,]
        #cross validation
        fit = randomForest(x = as.matrix(train[,-c(1,2)]), y = train$ADHD, 
                           ntree = 600)
        pred = ifelse(predict(fit, newdata = as.matrix(test[,-c(1,2)])) > 0.5, 1, 0)
        
        error[i] <- mean(pred != test$ADHD)
      }
      return(mean(error))
    }
    ```
    ```{r, message = FALSE, warning = FALSE, cache = TRUE, autodep = TRUE}
    set.seed(100)
    norm.mc.rf = mc.rf(data.norm)
    change.norm.mc.rf = mc.rf(data.change.norm)
    ACT.norm.mc.rf = mc.rf(data.ACT.norm)
    CON.norm.mc.rf = mc.rf(data.CON.norm)
    ATT.norm.mc.rf = mc.rf(data.ATT.norm)
    alpha.norm.mc.rf = mc.rf(data.alpha.norm)
    theta.norm.mc.rf = mc.rf(data.theta.norm)
    
    norm.mc.rf
    change.norm.mc.rf
    ACT.norm.mc.rf
    CON.norm.mc.rf
    ATT.norm.mc.rf
    alpha.norm.mc.rf
    theta.norm.mc.rf
    ```
    ```{r}
    #We observed a 20% increase in classification accuracy
    base
    ```


    Notes:
    + Sparse additive model is non-linear, and variables don't interact, generalization of linear regression with variable selection into the non-linear setting. Huge weakness: no way to design the functions such that the summation of the functions would give you a contour plot. Best for when data is sparse, i.e. p >> n.
    + Linear model is linear, all variables interact
    
    Cassie's notes after first analysis
    + Alpha and Theta analyses differently (which has highest accuracy rate)
    + Cassie has only looked @ Alpha (6-9 Hz), Theta is (3-7 Hz) and just included data for the sake of it
    + Attention Task instead of change scores (vs. baseline) - we should still keep change scores because there is a dependence between attention task and baseline
    + Non-normalized data
    + Visualization (topographic maps by James Long company): ERP Visualization
    + Training phase, is there any test data?
    + Q-Q Plot of residuals


















####Logistic Regression

    Model fitting
    ```{r}
    #Fit logistic model
    data2 = data[,c(1,2,grep("ALPHA", colnames(data)))]
    lg.mdl = glm(ADHD ~ . -ID-ADHD, data = data2, family = binomial)
    plot(data$ADHD, predict(lg.mdl, type = "response"))
    predict(lg.mdl, type = "response")
    
    table(predict(lg.mdl, type = "response"), data$ADHD)
    
    #data.ATT
    #data.ATT.norm
    #data.change
    #data.change.norm
    ```